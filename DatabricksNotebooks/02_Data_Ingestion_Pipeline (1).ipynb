{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b76c6e5-09b3-4a37-8b58-f16d3f07ec38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 CURRENT CONFIGURATION STATUS:\n   Storage Account: irishhealthdata ✅\n   Database: supply_chain_analysis ✅\n   MLflow Experiment: /Users/u1025325052@gmail.com/supply_chain_risk_prediction ✅\n   GNews API Key: ✅ CONFIGURED\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'schema_extra' has been renamed to 'json_schema_extra'\n  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 Azure Environment Configuration:\n   resource_group: irish-healthcare-agents-west-europe\n   databricks_workspace: irish-healthcare-db\n   storage_account: irishhealthdata\n   location: westeurope\n   container_name: supply-chain-data\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container already exists: supply-chain-data\n✅ Azure Storage connection established successfully!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database 'supply_chain_analysis' created\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bronze layer tables created\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The execution of this command did not finish successfully",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./01_Environment_Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7917026-f92d-4968-92ca-3e18736a19bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment imported from Notebook 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ports dimension table created with 17 major global ports\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>port_name</th><th>latitude</th><th>longitude</th><th>country</th><th>port_size</th><th>capacity_rating</th><th>region</th></tr></thead><tbody><tr><td>Shanghai</td><td>31.2304</td><td>121.4737</td><td>China</td><td>Major</td><td>Very High</td><td>Asia</td></tr><tr><td>Singapore</td><td>1.2644</td><td>103.822</td><td>Singapore</td><td>Major</td><td>Very High</td><td>Asia</td></tr><tr><td>Shenzhen</td><td>22.5431</td><td>114.0579</td><td>China</td><td>Major</td><td>High</td><td>Asia</td></tr><tr><td>Ningbo-Zhoushan</td><td>29.8686</td><td>121.5433</td><td>China</td><td>Major</td><td>High</td><td>Asia</td></tr><tr><td>Hong Kong</td><td>22.3193</td><td>114.1694</td><td>China</td><td>Major</td><td>High</td><td>Asia</td></tr><tr><td>Busan</td><td>35.1796</td><td>129.0756</td><td>South Korea</td><td>Major</td><td>High</td><td>Asia</td></tr><tr><td>Qingdao</td><td>36.0671</td><td>120.3826</td><td>China</td><td>Major</td><td>Medium</td><td>Asia</td></tr><tr><td>Rotterdam</td><td>51.9225</td><td>4.47917</td><td>Netherlands</td><td>Major</td><td>High</td><td>Europe</td></tr><tr><td>Antwerp</td><td>51.2291</td><td>4.4053</td><td>Belgium</td><td>Major</td><td>High</td><td>Europe</td></tr><tr><td>Hamburg</td><td>53.5511</td><td>9.9937</td><td>Germany</td><td>Major</td><td>High</td><td>Europe</td></tr><tr><td>Felixstowe</td><td>51.9617</td><td>1.3513</td><td>UK</td><td>Major</td><td>Medium</td><td>Europe</td></tr><tr><td>Valencia</td><td>39.4699</td><td>-0.3763</td><td>Spain</td><td>Major</td><td>Medium</td><td>Europe</td></tr><tr><td>Los Angeles</td><td>33.7175</td><td>-118.2675</td><td>USA</td><td>Major</td><td>High</td><td>North America</td></tr><tr><td>Long Beach</td><td>33.7623</td><td>-118.1954</td><td>USA</td><td>Major</td><td>High</td><td>North America</td></tr><tr><td>New York</td><td>40.6895</td><td>-74.1745</td><td>USA</td><td>Major</td><td>High</td><td>North America</td></tr><tr><td>Savannah</td><td>32.0814</td><td>-81.0914</td><td>USA</td><td>Major</td><td>Medium</td><td>North America</td></tr><tr><td>Vancouver</td><td>49.2827</td><td>-123.1207</td><td>Canada</td><td>Major</td><td>Medium</td><td>North America</td></tr><tr><td>Jebel Ali</td><td>25.0263</td><td>55.0564</td><td>UAE</td><td>Major</td><td>High</td><td>Middle East</td></tr><tr><td>Salalah</td><td>16.9344</td><td>54.0239</td><td>Oman</td><td>Major</td><td>Medium</td><td>Middle East</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Shanghai",
         31.2304,
         121.4737,
         "China",
         "Major",
         "Very High",
         "Asia"
        ],
        [
         "Singapore",
         1.2644,
         103.822,
         "Singapore",
         "Major",
         "Very High",
         "Asia"
        ],
        [
         "Shenzhen",
         22.5431,
         114.0579,
         "China",
         "Major",
         "High",
         "Asia"
        ],
        [
         "Ningbo-Zhoushan",
         29.8686,
         121.5433,
         "China",
         "Major",
         "High",
         "Asia"
        ],
        [
         "Hong Kong",
         22.3193,
         114.1694,
         "China",
         "Major",
         "High",
         "Asia"
        ],
        [
         "Busan",
         35.1796,
         129.0756,
         "South Korea",
         "Major",
         "High",
         "Asia"
        ],
        [
         "Qingdao",
         36.0671,
         120.3826,
         "China",
         "Major",
         "Medium",
         "Asia"
        ],
        [
         "Rotterdam",
         51.9225,
         4.47917,
         "Netherlands",
         "Major",
         "High",
         "Europe"
        ],
        [
         "Antwerp",
         51.2291,
         4.4053,
         "Belgium",
         "Major",
         "High",
         "Europe"
        ],
        [
         "Hamburg",
         53.5511,
         9.9937,
         "Germany",
         "Major",
         "High",
         "Europe"
        ],
        [
         "Felixstowe",
         51.9617,
         1.3513,
         "UK",
         "Major",
         "Medium",
         "Europe"
        ],
        [
         "Valencia",
         39.4699,
         -0.3763,
         "Spain",
         "Major",
         "Medium",
         "Europe"
        ],
        [
         "Los Angeles",
         33.7175,
         -118.2675,
         "USA",
         "Major",
         "High",
         "North America"
        ],
        [
         "Long Beach",
         33.7623,
         -118.1954,
         "USA",
         "Major",
         "High",
         "North America"
        ],
        [
         "New York",
         40.6895,
         -74.1745,
         "USA",
         "Major",
         "High",
         "North America"
        ],
        [
         "Savannah",
         32.0814,
         -81.0914,
         "USA",
         "Major",
         "Medium",
         "North America"
        ],
        [
         "Vancouver",
         49.2827,
         -123.1207,
         "Canada",
         "Major",
         "Medium",
         "North America"
        ],
        [
         "Jebel Ali",
         25.0263,
         55.0564,
         "UAE",
         "Major",
         "High",
         "Middle East"
        ],
        [
         "Salalah",
         16.9344,
         54.0239,
         "Oman",
         "Major",
         "Medium",
         "Middle East"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "port_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "port_size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "capacity_rating",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLflow experiment configured: /Users/u1025325052@gmail.com/supply_chain_risk_prediction\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Running Environment Validation...\n✅ Test 1: Database accessible\n✅ Test 2: 3 tables exist\n✅ Test 3: MLflow experiment configured\n✅ Test 4: Azure Storage connected\n✅ Test 5: Spark operations working\n\n\uD83C\uDFAF Validation Results: 5/5 tests passed\n\uD83D\uDE80 Environment is READY for data ingestion!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCBE Saving configuration with ACTUAL values:\n   • GNews API Key in config: ✅ PRESENT\n   • GNews API Key value: cd8e55ad6d...\n   • GDACS URL: https://www.gdacs.org/gdacsapi/api/events/geteventlist/SEARCH\n✅ Configuration saved to: /dbfs/FileStore/supply_chain/config.json\n\n\uD83D\uDD0D VERIFYING SAVED CONFIGURATION:\n   • GNews API Key saved: cd8e55ad6d...\n   • GDACS URL saved: https://www.gdacs.org/gdacsapi/api/events/geteventlist/SEARCH\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    \uD83C\uDF89 ENVIRONMENT SETUP COMPLETE!\n    \n    Next steps:\n    1. ✅ Replace GNews API key in the configuration\n    2. ➡️ Proceed to Notebook 2: Data Ingestion Pipeline\n    3. \uD83D\uDD27 Configure Azure OpenAI (optional for now)\n    \n    Your Azure Resources:\n    • Resource Group: irish-healthcare-agents-west-europe\n    • Databricks: irish-healthcare-db\n    • Storage: irishhealthdata\n    • Location: West Europe\n    \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 CURRENT CONFIGURATION STATUS:\n   Storage Account: irishhealthdata ✅\n   Database: supply_chain_analysis ✅\n   MLflow Experiment: /Users/u1025325052@gmail.com/supply_chain_risk_prediction ✅\n   GNews API Key: ✅ CONFIGURED\n"
     ]
    }
   ],
   "source": [
    "# Additional imports for data ingestion\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.sql.types import DoubleType, StringType, TimestampType, IntegerType, ArrayType\n",
    "\n",
    "print(\"✅ Environment imported from Notebook 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6adce0-cfb9-409e-8063-03840723dfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 USING DIRECT API CONFIGURATION:\n   • GNews API: ✅ CONFIGURED\n   • GDACS API: ✅ READY\n"
     ]
    }
   ],
   "source": [
    "GNEWS_API_KEY = \"\"  # ← REPLACE THIS WITH YOUR REAL GNEWS KEY!\n",
    "GDACS_URL = \"https://www.gdacs.org/gdacsapi/api/events/geteventlist/SEARCH\"\n",
    "\n",
    "print(\"\uD83D\uDD27 USING DIRECT API CONFIGURATION:\")\n",
    "print(f\"   • GNews API: {'✅ CONFIGURED' if GNEWS_API_KEY != 'YOUR_ACTUAL_GNEWS_API_KEY_HERE' else '❌ NOT CONFIGURED - REPLACE THE KEY!'}\")\n",
    "print(f\"   • GDACS API: ✅ READY\")\n",
    "\n",
    "if GNEWS_API_KEY == \"YOUR_ACTUAL_GNEWS_API_KEY_HERE\":\n",
    "    print(\"\\n\uD83D\uDEA8 ACTION REQUIRED:\")\n",
    "    print(\"   Please replace 'YOUR_ACTUAL_GNEWS_API_KEY_HERE' with your real GNews API key\")\n",
    "    print(\"   Get your free API key from: https://gnews.io/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc47aba6-75cc-4715-ba1f-68ae7bad9c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bronze tables created with explicit schemas\n"
     ]
    }
   ],
   "source": [
    "# Define consistent schemas for Delta tables\n",
    "gdacs_bronze_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"severity\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"start_date\", TimestampType(), True),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"alert_level\", StringType(), True),\n",
    "    StructField(\"population_affected\", IntegerType(), True),\n",
    "    StructField(\"insert_timestamp\", TimestampType(), True),\n",
    "    StructField(\"data_source\", StringType(), True)\n",
    "])\n",
    "\n",
    "news_bronze_schema = StructType([\n",
    "    StructField(\"article_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"published_at\", TimestampType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"keywords\", ArrayType(StringType()), True),\n",
    "    StructField(\"sentiment_score\", DoubleType(), True),\n",
    "    StructField(\"insert_timestamp\", TimestampType(), True),\n",
    "    StructField(\"data_source\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create tables with explicit schemas\n",
    "def create_bronze_tables_with_schema():\n",
    "    \"\"\"Create bronze tables with explicit schemas\"\"\"\n",
    "    \n",
    "    spark.sql(\"DROP TABLE IF EXISTS supply_chain_analysis.bronze_gdacs_alerts\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS supply_chain_analysis.bronze_supply_chain_news\")\n",
    "    \n",
    "    spark.createDataFrame([], gdacs_bronze_schema) \\\n",
    "        .write.format(\"delta\").saveAsTable(\"supply_chain_analysis.bronze_gdacs_alerts\")\n",
    "    \n",
    "    spark.createDataFrame([], news_bronze_schema) \\\n",
    "        .write.format(\"delta\").saveAsTable(\"supply_chain_analysis.bronze_supply_chain_news\")\n",
    "    \n",
    "    print(\"✅ Bronze tables created with explicit schemas\")\n",
    "\n",
    "create_bronze_tables_with_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050cbead-2955-4a59-a930-a885d581e463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_real_gdacs_disasters():\n",
    "    \"\"\"Fetch REAL disaster data from GDACS API - No fallback to sample data\"\"\"\n",
    "    try:\n",
    "        print(\"\uD83C\uDF2A️ Fetching REAL disaster data from GDACS API...\")\n",
    "        \n",
    "        # Parameters for recent events - broader search to get real data\n",
    "        params = {\n",
    "            \"fromDate\": (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\"),  # Last 30 days\n",
    "            \"toDate\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"alertLevel\": \"Green;Orange;Red\",  # Include all alert levels\n",
    "            \"eventType\": \"EQ;TC;FL;VO;DR;WF\"  # All event types\n",
    "        }\n",
    "        \n",
    "        print(f\"   • API Endpoint: {GDACS_URL}\")\n",
    "        print(f\"   • Search Period: Last 30 days\")\n",
    "        print(f\"   • Alert Levels: All\")\n",
    "        \n",
    "        response = requests.get(GDACS_URL, params=params, timeout=30)\n",
    "        print(f\"   • Response Status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "            print(f\"   • Found {len(features)} features in API response\")\n",
    "            \n",
    "            events = []\n",
    "            for i, feature in enumerate(features):\n",
    "                props = feature.get('properties', {})\n",
    "                geometry = feature.get('geometry', {})\n",
    "                \n",
    "                # Parse coordinates\n",
    "                coords = geometry.get('coordinates', [0, 0])\n",
    "                longitude = float(coords[0]) if len(coords) > 0 else 0.0\n",
    "                latitude = float(coords[1]) if len(coords) > 1 else 0.0\n",
    "                \n",
    "                # Parse dates\n",
    "                start_date_str = props.get('fromdate', '')\n",
    "                end_date_str = props.get('todate', '')\n",
    "                \n",
    "                try:\n",
    "                    start_date = datetime.fromisoformat(start_date_str.replace('Z', '+00:00')) if start_date_str else datetime.now()\n",
    "                    end_date = datetime.fromisoformat(end_date_str.replace('Z', '+00:00')) if end_date_str else datetime.now()\n",
    "                except:\n",
    "                    start_date = datetime.now()\n",
    "                    end_date = datetime.now()\n",
    "                \n",
    "                # Only include events with valid coordinates\n",
    "                if latitude != 0.0 and longitude != 0.0:\n",
    "                    event = (\n",
    "                        props.get('eventid', f\"GDACS_{i}\"),\n",
    "                        props.get('eventtype', 'Unknown'),\n",
    "                        props.get('name', 'Unnamed Event'),\n",
    "                        float(latitude),\n",
    "                        float(longitude),\n",
    "                        props.get('severity', 'Unknown'),\n",
    "                        props.get('country', 'Unknown'),\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        props.get('alertlevel', 'Green'),\n",
    "                        int(props.get('population', 0)),\n",
    "                        datetime.now(),\n",
    "                        'GDACS_API_REAL'\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            \n",
    "            if events:\n",
    "                df = spark.createDataFrame(events, gdacs_bronze_schema)\n",
    "                print(f\"✅ Successfully collected {len(events)} REAL disaster events\")\n",
    "                \n",
    "                # Show statistics about collected data\n",
    "                event_types = df.select(\"event_type\").distinct().collect()\n",
    "                countries = df.select(\"country\").distinct().collect()\n",
    "                alert_levels = df.select(\"alert_level\").distinct().collect()\n",
    "                \n",
    "                print(f\"   • Event Types: {[row.event_type for row in event_types]}\")\n",
    "                print(f\"   • Countries: {[row.country for row in countries[:5]]}{'...' if len(countries) > 5 else ''}\")\n",
    "                print(f\"   • Alert Levels: {[row.alert_level for row in alert_levels]}\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"❌ No valid events found in GDACS API response\")\n",
    "                print(\"   • This could mean:\")\n",
    "                print(\"     - No recent disasters in the search criteria\")\n",
    "                print(\"     - API response format changed\")\n",
    "                print(\"     - Network/connectivity issues\")\n",
    "                \n",
    "                # Create empty DataFrame with proper schema instead of sample data\n",
    "                empty_df = spark.createDataFrame([], gdacs_bronze_schema)\n",
    "                print(\"   • Returning empty DataFrame to maintain data integrity\")\n",
    "                return empty_df\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ GDACS API returned error status: {response.status_code}\")\n",
    "            if hasattr(response, 'text'):\n",
    "                print(f\"   • Response: {response.text[:200]}...\")\n",
    "            \n",
    "            # Return empty DataFrame instead of sample data\n",
    "            empty_df = spark.createDataFrame([], gdacs_bronze_schema)\n",
    "            return empty_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ GDACS API error: {str(e)}\")\n",
    "        \n",
    "        # Return empty DataFrame instead of sample data\n",
    "        empty_df = spark.createDataFrame([], gdacs_bronze_schema)\n",
    "        return empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db48c87a-51d6-4364-a975-cfc127b4a7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## \uD83D\uDCF0 REAL GNews Data Collection - COMPLETELY FIXED\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def fetch_real_supply_chain_news():\n",
    "    \"\"\"Fetch REAL news from GNews API - COMPLETELY FIXED VERSION\"\"\"\n",
    "    # Check if API key is properly configured\n",
    "    if GNEWS_API_KEY == \"YOUR_ACTUAL_GNEWS_API_KEY_HERE\":\n",
    "        print(\"❌ GNews API key not configured\")\n",
    "        print(\"   • Please replace 'YOUR_ACTUAL_GNEWS_API_KEY_HERE' with your real API key\")\n",
    "        empty_df = spark.createDataFrame([], news_bronze_schema)\n",
    "        return empty_df\n",
    "    \n",
    "    try:\n",
    "        print(\"\uD83D\uDCF0 Fetching REAL supply chain news from GNews API...\")\n",
    "        print(f\"   • API Key: {GNEWS_API_KEY[:8]}...{GNEWS_API_KEY[-4:]}\")\n",
    "        \n",
    "        url = \"https://gnews.io/api/v4/search\"\n",
    "        all_articles = []\n",
    "        \n",
    "        # Search queries for supply chain issues\n",
    "        search_queries = [\n",
    "            \"port congestion shipping\",\n",
    "            \"supply chain disruption\",\n",
    "            \"maritime logistics\",\n",
    "            \"shipping delays\", \n",
    "            \"container shipping\"\n",
    "        ]\n",
    "        \n",
    "        for query in search_queries:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'lang': 'en',\n",
    "                'max': 10,\n",
    "                'apikey': GNEWS_API_KEY\n",
    "            }\n",
    "            \n",
    "            print(f\"   • Searching: '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, params=params, timeout=15)\n",
    "                print(f\"      • Status: {response.status_code}\")\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    articles = data.get('articles', [])\n",
    "                    all_articles.extend(articles)\n",
    "                    print(f\"      ✅ Found {len(articles)} articles\")\n",
    "                    \n",
    "                    if articles:\n",
    "                        title = articles[0].get('title', 'No title')\n",
    "                        print(f\"      • Sample: '{title[:60]}...'\")\n",
    "                    \n",
    "                    time.sleep(1)  # Rate limiting\n",
    "                    \n",
    "                elif response.status_code == 401:\n",
    "                    print(\"      ❌ 401 Unauthorized - Invalid API key\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"      ❌ API error: {response.status_code}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      ⚠️ Request failed: {str(e)}\")\n",
    "        \n",
    "        if all_articles:\n",
    "            processed_articles = []\n",
    "            for i, article in enumerate(all_articles):\n",
    "                try:\n",
    "                    # Enhanced sentiment analysis - COMPLETELY FIXED\n",
    "                    content = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "                    sentiment_score = fixed_sentiment_analysis(content)\n",
    "                    \n",
    "                    # Parse date safely\n",
    "                    published_at = parse_news_date(article.get('publishedAt', ''))\n",
    "                    \n",
    "                    # Extract keywords safely\n",
    "                    keywords = extract_supply_chain_keywords(content)\n",
    "                    \n",
    "                    # Create article data\n",
    "                    news_article = (\n",
    "                        f\"REAL_NEWS_{i}_{int(datetime.now().timestamp())}\",  # article_id\n",
    "                        article.get('title', 'No Title'),                    # title\n",
    "                        article.get('description', 'No Description'),        # description\n",
    "                        article.get('content', 'No Content'),                # content\n",
    "                        published_at,                                        # published_at\n",
    "                        article.get('source', {}).get('name', 'Unknown Source'),  # source\n",
    "                        article.get('url', ''),                             # url\n",
    "                        keywords,                                           # keywords\n",
    "                        float(sentiment_score),                             # sentiment_score\n",
    "                        datetime.now(),                                     # insert_timestamp\n",
    "                        'GNEWS_API_REAL'                                    # data_source\n",
    "                    )\n",
    "                    processed_articles.append(news_article)\n",
    "                except Exception as e:\n",
    "                    print(f\"      ⚠️ Failed to process article {i}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if processed_articles:\n",
    "                # Create DataFrame with explicit schema\n",
    "                df = spark.createDataFrame(processed_articles, news_bronze_schema)\n",
    "                print(f\"✅ Processed {len(processed_articles)} REAL news articles\")\n",
    "                \n",
    "                # Show statistics\n",
    "                sources = df.select(\"source\").distinct().collect()\n",
    "                avg_sentiment = df.agg(avg(\"sentiment_score\")).collect()[0][0] or 0.0\n",
    "                \n",
    "                print(f\"   • Sources: {[row.source for row in sources[:3]]}{'...' if len(sources) > 3 else ''}\")\n",
    "                print(f\"   • Avg Sentiment: {avg_sentiment:.2f}\")\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"❌ No articles could be processed successfully\")\n",
    "                empty_df = spark.createDataFrame([], news_bronze_schema)\n",
    "                return empty_df\n",
    "        else:\n",
    "            print(\"❌ No articles found in GNews API response\")\n",
    "            empty_df = spark.createDataFrame([], news_bronze_schema)\n",
    "            return empty_df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ GNews API error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"   • Full error: {traceback.format_exc()}\")\n",
    "        empty_df = spark.createDataFrame([], news_bronze_schema)\n",
    "        return empty_df\n",
    "\n",
    "def fixed_sentiment_analysis(text):\n",
    "    \"\"\"COMPLETELY FIXED sentiment analysis without min() error\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 0.0\n",
    "        \n",
    "    # Negative impact keywords\n",
    "    severe_negative = {\n",
    "        'strike': -1.0, 'closed': -0.9, 'shutdown': -0.9, 'crisis': -0.8,\n",
    "        'collapse': -0.8, 'emergency': -0.7, 'bankruptcy': -0.9, 'war': -1.0\n",
    "    }\n",
    "    \n",
    "    negative_keywords = {\n",
    "        'delay': -0.6, 'congestion': -0.6, 'disruption': -0.7, 'shortage': -0.6,\n",
    "        'backlog': -0.5, 'bottleneck': -0.5, 'protest': -0.6, 'waiting': -0.4,\n",
    "        'slowdown': -0.5, 'problem': -0.4, 'issue': -0.4, 'challenge': -0.3\n",
    "    }\n",
    "    \n",
    "    positive_keywords = {\n",
    "        'recovery': 0.7, 'improved': 0.6, 'efficient': 0.5, 'solution': 0.5,\n",
    "        'resolved': 0.6, 'normalized': 0.5, 'reopened': 0.6, 'cleared': 0.5,\n",
    "        'growth': 0.4, 'expansion': 0.4, 'success': 0.5, 'breakthrough': 0.6\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    sentiment_score = 0.0\n",
    "    keyword_count = 0\n",
    "    \n",
    "    # Check severe negative keywords\n",
    "    for word, weight in severe_negative.items():\n",
    "        if word in text_lower:\n",
    "            sentiment_score += weight\n",
    "            keyword_count += 1\n",
    "    \n",
    "    # Check regular negative keywords\n",
    "    for word, weight in negative_keywords.items():\n",
    "        if word in text_lower:\n",
    "            sentiment_score += weight\n",
    "            keyword_count += 1\n",
    "    \n",
    "    # Check positive keywords\n",
    "    for word, weight in positive_keywords.items():\n",
    "        if word in text_lower:\n",
    "            sentiment_score += weight\n",
    "            keyword_count += 1\n",
    "    \n",
    "    # Normalize score - COMPLETELY FIXED: No min() with multiple arguments\n",
    "    if keyword_count > 0:\n",
    "        final_score = sentiment_score / keyword_count\n",
    "        \n",
    "        # Clamp between -1 and 1 using proper if-else (no min() with multiple args)\n",
    "        if final_score > 1.0:\n",
    "            return 1.0\n",
    "        elif final_score < -1.0:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return final_score\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def extract_supply_chain_keywords(text):\n",
    "    \"\"\"Extract relevant supply chain keywords\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "        \n",
    "    keywords = [\n",
    "        'port', 'shipping', 'logistics', 'supply chain', 'cargo', 'container',\n",
    "        'maritime', 'freight', 'vessel', 'terminal', 'customs', 'export', 'import',\n",
    "        'trade', 'shipping line', 'carrier', 'warehouse', 'distribution',\n",
    "        'transport', 'delivery', 'inventory', 'procurement'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found_keywords = []\n",
    "    \n",
    "    for kw in keywords:\n",
    "        if kw in text_lower:\n",
    "            found_keywords.append(kw)\n",
    "    \n",
    "    # Return top 8 keywords maximum\n",
    "    return found_keywords[:8]\n",
    "\n",
    "def parse_news_date(date_string):\n",
    "    \"\"\"Parse news date string safely\"\"\"\n",
    "    try:\n",
    "        if date_string:\n",
    "            # Handle various date formats\n",
    "            if 'T' in date_string and 'Z' in date_string:\n",
    "                return datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "            else:\n",
    "                # Try other common formats\n",
    "                return datetime.fromisoformat(date_string)\n",
    "        else:\n",
    "            return datetime.now()\n",
    "    except:\n",
    "        return datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0354ecde-9d01-423b-893f-03503b9d6529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 STARTING REAL DATA INGESTION PIPELINE...\n============================================================\n\uD83D\uDCDD NOTE: Using REAL APIs only - No sample data generation\n\uD83D\uDD2C MLflow Run Started: 67a32be91f8c46108d00eb9befc9d30c\n\n1. Ingesting GDACS Disaster Data...\n\uD83C\uDF2A️ Fetching REAL disaster data from GDACS API...\n   • API Endpoint: https://www.gdacs.org/gdacsapi/api/events/geteventlist/SEARCH\n   • Search Period: Last 30 days\n   • Alert Levels: All\n   • Response Status: 200\n   • Found 100 features in API response\n✅ Successfully collected 100 REAL disaster events\n   • Event Types: ['TC', 'EQ', 'WF', 'FL']\n   • Countries: ['Russia', 'Philippines', 'Fiji', 'Northern East Pacfic Rise', 'South Of Panama']...\n   • Alert Levels: ['Orange', 'Green']\n   ✅ Stored 100 REAL disaster events (2.9s)\n\n2. Ingesting Supply Chain News...\n\uD83D\uDCF0 Fetching REAL supply chain news from GNews API...\n   • API Key: cd8e55ad...c34d\n   • Searching: 'port congestion shipping'\n      • Status: 200\n      ✅ Found 0 articles\n   • Searching: 'supply chain disruption'\n      • Status: 200\n      ✅ Found 10 articles\n      • Sample: 'Continuous improvement in your supply chain has never been m...'\n   • Searching: 'maritime logistics'\n      • Status: 200\n      ✅ Found 10 articles\n      • Sample: 'Samsung heavy industries joins India's shipbuilding push wit...'\n   • Searching: 'shipping delays'\n      • Status: 200\n      ✅ Found 9 articles\n      • Sample: 'USPS deadline alert: Last chance to ship holiday cards and g...'\n   • Searching: 'container shipping'\n      • Status: 200\n      ✅ Found 10 articles\n      • Sample: 'Falling ocean shipping rates put carrier profits at risk, an...'\n✅ Processed 39 REAL news articles\n   • Sources: ['Wilmington News Journal', 'Fast Company', 'The Hindu Business Line']...\n   • Avg Sentiment: -0.33\n   ✅ Stored 39 REAL news articles (8.6s)\n\uD83C\uDF89 REAL DATA INGESTION COMPLETED! Total data points: 139\n"
     ]
    }
   ],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## \uD83D\uDE80 Data Ingestion Execution - WITH FIXED SENTIMENT\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\uD83D\uDE80 STARTING REAL DATA INGESTION PIPELINE...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDCDD NOTE: Using REAL APIs only - No sample data generation\")\n",
    "\n",
    "# Store MLflow run ID for later use\n",
    "mlflow_run_id = None\n",
    "\n",
    "try:\n",
    "    # Start MLflow run to track data collection\n",
    "    with mlflow.start_run(run_name=\"real_data_ingestion_fixed_sentiment\") as run:\n",
    "        mlflow_run_id = run.info.run_id\n",
    "        print(f\"\uD83D\uDD2C MLflow Run Started: {mlflow_run_id}\")\n",
    "        \n",
    "        # Track ingestion parameters\n",
    "        mlflow.log_param(\"data_source_gdacs\", \"API_REAL\")\n",
    "        mlflow.log_param(\"data_source_news\", \"API_REAL\")\n",
    "        mlflow.log_param(\"ingestion_timestamp\", datetime.now().isoformat())\n",
    "        mlflow.log_param(\"sentiment_analysis\", \"FIXED_VERSION\")\n",
    "        \n",
    "        print(\"\\n1. Ingesting GDACS Disaster Data...\")\n",
    "        start_time = time.time()\n",
    "        gdacs_df = fetch_real_gdacs_disasters()\n",
    "        gdacs_duration = time.time() - start_time\n",
    "        \n",
    "        gdacs_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_analysis.bronze_gdacs_alerts\")\n",
    "        mlflow.log_metric(\"gdacs_events_collected\", gdacs_df.count())\n",
    "        mlflow.log_metric(\"gdacs_ingestion_seconds\", gdacs_duration)\n",
    "        \n",
    "        print(f\"   ✅ Stored {gdacs_df.count()} REAL disaster events ({gdacs_duration:.1f}s)\")\n",
    "        \n",
    "        print(\"\\n2. Ingesting Supply Chain News...\")\n",
    "        start_time = time.time()\n",
    "        news_df = fetch_real_supply_chain_news()  # This now uses the FIXED version\n",
    "        news_duration = time.time() - start_time\n",
    "        \n",
    "        news_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_analysis.bronze_supply_chain_news\")\n",
    "        mlflow.log_metric(\"news_articles_collected\", news_df.count())\n",
    "        mlflow.log_metric(\"news_ingestion_seconds\", news_duration)\n",
    "        \n",
    "        print(f\"   ✅ Stored {news_df.count()} REAL news articles ({news_duration:.1f}s)\")\n",
    "        \n",
    "        # Log additional metrics\n",
    "        total_data = gdacs_df.count() + news_df.count()\n",
    "        mlflow.log_metric(\"total_data_points\", total_data)\n",
    "        mlflow.log_metric(\"overall_ingestion_seconds\", gdacs_duration + news_duration)\n",
    "        \n",
    "        print(f\"\uD83C\uDF89 REAL DATA INGESTION COMPLETED! Total data points: {total_data}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ MLflow run error: {e}\")\n",
    "    print(\"\uD83D\uDD04 Continuing with data ingestion without MLflow...\")\n",
    "    \n",
    "    print(\"\\n1. Ingesting GDACS Disaster Data...\")\n",
    "    gdacs_df = fetch_real_gdacs_disasters()\n",
    "    gdacs_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_analysis.bronze_gdacs_alerts\")\n",
    "    print(f\"   ✅ Stored {gdacs_df.count()} REAL disaster events\")\n",
    "    \n",
    "    print(\"\\n2. Ingesting Supply Chain News...\")\n",
    "    news_df = fetch_real_supply_chain_news()  # This now uses the FIXED version\n",
    "    news_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_analysis.bronze_supply_chain_news\")\n",
    "    print(f\"   ✅ Stored {news_df.count()} REAL news articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56fb345-ac4d-4803-9c6a-dd24f93e3914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA REAL DATA QUALITY ANALYSIS\n==================================================\n\uD83C\uDF2A️ GDACS Disaster Data:\n   • Total Events: 100\n   • Event Types: ['EQ', 'WF', 'TC', 'FL']\n   • Countries: ['Russia', 'Philippines', 'South Of Kermadec Islands']...\n   • Data Source: GDACS_API_REAL\n\n\uD83D\uDCF0 Supply Chain News:\n   • Total Articles: 39\n   • Sources: ['Wilmington News Journal', 'Fast Company', 'The Hindu Business Line']...\n   • Avg Sentiment: -0.33\n   • Data Source: GNEWS_API_REAL\n"
     ]
    }
   ],
   "source": [
    "def analyze_real_data_quality():\n",
    "    \"\"\"Analyze quality of REAL ingested data\"\"\"\n",
    "    print(\"\uD83D\uDCCA REAL DATA QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # GDACS Data Analysis\n",
    "    gdacs_table = spark.table(\"supply_chain_analysis.bronze_gdacs_alerts\")\n",
    "    gdacs_count = gdacs_table.count()\n",
    "    \n",
    "    print(\"\uD83C\uDF2A️ GDACS Disaster Data:\")\n",
    "    print(f\"   • Total Events: {gdacs_count}\")\n",
    "    \n",
    "    if gdacs_count > 0:\n",
    "        event_types = gdacs_table.select(\"event_type\").distinct().collect()\n",
    "        countries = gdacs_table.select(\"country\").distinct().collect()\n",
    "        data_source = gdacs_table.select(\"data_source\").first()[0]\n",
    "        \n",
    "        print(f\"   • Event Types: {[row.event_type for row in event_types]}\")\n",
    "        print(f\"   • Countries: {[row.country for row in countries[:3]]}{'...' if len(countries) > 3 else ''}\")\n",
    "        print(f\"   • Data Source: {data_source}\")\n",
    "    else:\n",
    "        print(\"   • ⚠️ No disaster data collected from API\")\n",
    "    \n",
    "    # News Data Analysis\n",
    "    news_table = spark.table(\"supply_chain_analysis.bronze_supply_chain_news\")\n",
    "    news_count = news_table.count()\n",
    "    \n",
    "    print(\"\\n\uD83D\uDCF0 Supply Chain News:\")\n",
    "    print(f\"   • Total Articles: {news_count}\")\n",
    "    \n",
    "    if news_count > 0:\n",
    "        sources = news_table.select(\"source\").distinct().collect()\n",
    "        avg_sentiment = news_table.agg(avg(\"sentiment_score\")).collect()[0][0]\n",
    "        data_source = news_table.select(\"data_source\").first()[0]\n",
    "        \n",
    "        print(f\"   • Sources: {[row.source for row in sources[:3]]}{'...' if len(sources) > 3 else ''}\")\n",
    "        print(f\"   • Avg Sentiment: {avg_sentiment:.2f}\")\n",
    "        print(f\"   • Data Source: {data_source}\")\n",
    "    else:\n",
    "        print(\"   • ⚠️ No news data collected from API\")\n",
    "    \n",
    "    return gdacs_count, news_count\n",
    "\n",
    "gdacs_count, news_count = analyze_real_data_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70924181-a9b1-4870-b216-35ba32dde287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D PREVIEW OF REAL DATA COLLECTED\n==================================================\n\n\uD83C\uDF2A️ Recent Disaster Alerts:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>event_id</th><th>event_type</th><th>event_name</th><th>country</th><th>alert_level</th><th>start_date</th></tr></thead><tbody><tr><td>1503491</td><td>EQ</td><td>Earthquake in Fiji</td><td>Fiji</td><td>Green</td><td>2025-10-04T18:50:54Z</td></tr><tr><td>1503487</td><td>EQ</td><td>Earthquake in Russia</td><td>Russia</td><td>Green</td><td>2025-10-04T17:43:53Z</td></tr><tr><td>1503482</td><td>EQ</td><td>Earthquake in Off East Coast Of Kamchatka</td><td>Off East Coast Of Kamchatka</td><td>Green</td><td>2025-10-04T17:38:01Z</td></tr><tr><td>1503467</td><td>EQ</td><td>Earthquake in Japan</td><td>Japan</td><td>Green</td><td>2025-10-04T15:21:09Z</td></tr><tr><td>1503456</td><td>EQ</td><td>Earthquake in Japan</td><td>Japan</td><td>Green</td><td>2025-10-04T14:15:19Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1503491",
         "EQ",
         "Earthquake in Fiji",
         "Fiji",
         "Green",
         "2025-10-04T18:50:54Z"
        ],
        [
         "1503487",
         "EQ",
         "Earthquake in Russia",
         "Russia",
         "Green",
         "2025-10-04T17:43:53Z"
        ],
        [
         "1503482",
         "EQ",
         "Earthquake in Off East Coast Of Kamchatka",
         "Off East Coast Of Kamchatka",
         "Green",
         "2025-10-04T17:38:01Z"
        ],
        [
         "1503467",
         "EQ",
         "Earthquake in Japan",
         "Japan",
         "Green",
         "2025-10-04T15:21:09Z"
        ],
        [
         "1503456",
         "EQ",
         "Earthquake in Japan",
         "Japan",
         "Green",
         "2025-10-04T14:15:19Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "event_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "event_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "event_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "alert_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCF0 Recent Supply Chain News:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>article_id</th><th>title</th><th>source</th><th>sentiment_score</th><th>published_at</th></tr></thead><tbody><tr><td>REAL_NEWS_29_1759611147</td><td>Falling ocean shipping rates put carrier profits at risk, analysts say</td><td>The Economic Times</td><td>-0.9</td><td>2025-10-04T05:01:00Z</td></tr><tr><td>REAL_NEWS_30_1759611147</td><td>China eyes Arctic shortcut as top container lines stay away</td><td>The Economic Times</td><td>0.0</td><td>2025-10-04T04:36:00Z</td></tr><tr><td>REAL_NEWS_31_1759611147</td><td>Falling ocean shipping rates put carrier profits at risk, analysts say</td><td>Reuters</td><td>0.0</td><td>2025-10-03T18:37:38Z</td></tr><tr><td>REAL_NEWS_32_1759611147</td><td>LNG row splits Europe as US challenges net-zero shipping deal</td><td>Euractiv</td><td>-0.3</td><td>2025-10-01T04:00:19Z</td></tr><tr><td>REAL_NEWS_33_1759611147</td><td>Global container shipping lines making waves</td><td>The Hindu Business Line</td><td>0.0</td><td>2025-09-30T16:05:59Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "REAL_NEWS_29_1759611147",
         "Falling ocean shipping rates put carrier profits at risk, analysts say",
         "The Economic Times",
         -0.9,
         "2025-10-04T05:01:00Z"
        ],
        [
         "REAL_NEWS_30_1759611147",
         "China eyes Arctic shortcut as top container lines stay away",
         "The Economic Times",
         0.0,
         "2025-10-04T04:36:00Z"
        ],
        [
         "REAL_NEWS_31_1759611147",
         "Falling ocean shipping rates put carrier profits at risk, analysts say",
         "Reuters",
         0.0,
         "2025-10-03T18:37:38Z"
        ],
        [
         "REAL_NEWS_32_1759611147",
         "LNG row splits Europe as US challenges net-zero shipping deal",
         "Euractiv",
         -0.3,
         "2025-10-01T04:00:19Z"
        ],
        [
         "REAL_NEWS_33_1759611147",
         "Global container shipping lines making waves",
         "The Hindu Business Line",
         0.0,
         "2025-09-30T16:05:59Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "article_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "published_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D PREVIEW OF REAL DATA COLLECTED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if gdacs_count > 0:\n",
    "    print(\"\\n\uD83C\uDF2A️ Recent Disaster Alerts:\")\n",
    "    display(spark.sql(\"\"\"\n",
    "    SELECT event_id, event_type, event_name, country, alert_level, start_date \n",
    "    FROM supply_chain_analysis.bronze_gdacs_alerts \n",
    "    ORDER BY start_date DESC \n",
    "    LIMIT 5\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(\"\\n\uD83C\uDF2A️ No disaster data available from GDACS API\")\n",
    "\n",
    "if news_count > 0:\n",
    "    print(\"\\n\uD83D\uDCF0 Recent Supply Chain News:\")\n",
    "    display(spark.sql(\"\"\"\n",
    "    SELECT article_id, title, source, sentiment_score, published_at \n",
    "    FROM supply_chain_analysis.bronze_supply_chain_news \n",
    "    ORDER BY published_at DESC \n",
    "    LIMIT 5\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(\"\\n\uD83D\uDCF0 No news data available from GNews API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c22dc41-4f76-4598-823a-8f12a0a3a69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration updated with REAL data ingestion results\n"
     ]
    }
   ],
   "source": [
    "# Update configuration with REAL ingestion results\n",
    "config_path = \"/dbfs/FileStore/supply_chain/config.json\"\n",
    "\n",
    "try:\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    config['ingestion_completed'] = True\n",
    "    config['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "    config['gdacs_events'] = gdacs_count\n",
    "    config['news_articles'] = news_count\n",
    "    config['real_data_only'] = True\n",
    "    config['sample_data_used'] = False\n",
    "    \n",
    "    if mlflow_run_id:\n",
    "        config['mlflow_run_id'] = mlflow_run_id\n",
    "    \n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(\"✅ Configuration updated with REAL data ingestion results\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to update configuration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb76e685-2db2-450e-aa9d-4bc435ccdb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83C\uDF89 REAL DATA INGESTION COMPLETED!\n\n\uD83D\uDCCA REAL DATA SUMMARY:\n• Disaster Alerts: 100 events\n• News Articles: 39 articles\n• Data Source: ✅ REAL APIs ONLY\n• Sample Data: ❌ NOT USED\n• Data Integrity: ✅ MAINTAINED\n\n\uD83D\uDD27 API PERFORMANCE:\n• GDACS API: ✅ DATA \n• GNews API: ✅ DATA\n\n\uD83D\uDCC8 NEXT STEPS:\n• Proceed to Notebook 3 for Feature Engineering\n• Even with limited data, we can build the ML pipeline\n• Real-world data scenarios include empty/limited responses\n• System designed for real production use cases\n\n\uD83D\uDCA1 PRODUCTION READY:\n• No synthetic data contamination\n• Real API error handling\n• Empty dataset management\n• Production-grade data pipeline\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\"\"\n",
    "\uD83C\uDF89 REAL DATA INGESTION COMPLETED!\n",
    "\n",
    "\uD83D\uDCCA REAL DATA SUMMARY:\n",
    "• Disaster Alerts: {} events\n",
    "• News Articles: {} articles\n",
    "• Data Source: ✅ REAL APIs ONLY\n",
    "• Sample Data: ❌ NOT USED\n",
    "• Data Integrity: ✅ MAINTAINED\n",
    "\n",
    "\uD83D\uDD27 API PERFORMANCE:\n",
    "• GDACS API: {} \n",
    "• GNews API: {}\n",
    "\n",
    "\uD83D\uDCC8 NEXT STEPS:\n",
    "• Proceed to Notebook 3 for Feature Engineering\n",
    "• Even with limited data, we can build the ML pipeline\n",
    "• Real-world data scenarios include empty/limited responses\n",
    "• System designed for real production use cases\n",
    "\n",
    "\uD83D\uDCA1 PRODUCTION READY:\n",
    "• No synthetic data contamination\n",
    "• Real API error handling\n",
    "• Empty dataset management\n",
    "• Production-grade data pipeline\n",
    "\"\"\".format(\n",
    "    gdacs_count, news_count,\n",
    "    \"✅ DATA\" if gdacs_count > 0 else \"⚠️ NO DATA\",\n",
    "    \"✅ DATA\" if news_count > 0 else \"⚠️ NO DATA\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c006633-75f6-4786-83c6-82ee96325617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Data_Ingestion_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}