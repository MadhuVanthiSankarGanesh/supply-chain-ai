# ğŸš¢ AI Supply Chain Intelligence Platform

An **end-to-end intelligent supply chain monitoring system** that leverages **Databricks, Azure OpenAI, and real-time data streams** to provide actionable insights into global logistics risks and disruptions.

## ğŸŒ Overview
This platform integrates **machine learning, natural language processing, and geospatial analytics** to assess and visualize supply chain vulnerabilities. It continuously monitors real-world eventsâ€”such as natural disasters, political instability, and trade disruptionsâ€”to support data-driven logistics decision-making.

---

## âœ¨ Key Features
- ğŸ¤– **AI-Powered Risk Analysis** â€” Automated detection and classification of supply chain risks using Azure OpenAI and LangChain.  
- ğŸ—ºï¸ **Interactive Route Analysis** â€” Real-time mapping of supply routes with geospatial visualization and risk overlays.  
- ğŸ“Š **Dynamic Dashboards** â€” Streamlit-powered dashboards for intuitive monitoring of key metrics.  
- ğŸŒªï¸ **Disaster Intelligence** â€” Integration with GDACS and GNews APIs for live natural disaster and news event tracking.  
- ğŸ“„ **Automated Reporting** â€” Generates daily and on-demand analytical summaries for stakeholders.

---

## ğŸ§  Tech Stack & Expertise

### ğŸ’» Data Engineering & Processing
- **Azure Databricks** â€” End-to-end data pipelines for ingestion, transformation, and feature engineering.  
- **Delta Lake** â€” Unified batch and streaming data storage with version control.  
- **PySpark** â€” Scalable data processing for multi-source integration and transformation.  

### ğŸ¤– Artificial Intelligence & Analytics
- **Azure OpenAI + LangChain** â€” Risk analysis and event summarization using LLMs.  
- **MLflow** â€” Experiment tracking and model management on Databricks.  
- **Pandas / NumPy** â€” Lightweight data wrangling and analytics during exploratory phases.

### ğŸ§± Cloud Infrastructure
- **Azure Cloud** â€” Data orchestration, model deployment, and environment scalability.  
- **Databricks Notebooks** â€” Modularized workflows for each pipeline stage:
  - `01_Environment_Setup.ipynb` â€” Cluster setup and environment configuration  
  - `02_Data_Ingestion_Pipeline.ipynb` â€” Data ingestion and Delta Lake integration  
  - `03_FeatureEngineering_RiskAnalysis.ipynb` â€” Feature engineering and ML-based risk computation  

### ğŸŒ APIs & Real-Time Data
- **GDACS API** â€” Natural disaster alerts and event metadata.  
- **GNews API** â€” News-based supply chain event extraction and sentiment tagging.  

### ğŸ¨ Visualization & Deployment
- **Streamlit** â€” Interactive visualization and deployment of AI dashboards.  
- **Plotly / Altair** â€” Advanced analytics and geospatial visualizations.  

---

## ğŸš€ Live Demo
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://supply-chain-ai-smchpstajndwkv8jjb6zwq.streamlit.app/)

---

## ğŸ§© Project Workflow
1. **Environment Setup** â€” Configure Databricks workspace, clusters, and data mounts.  
2. **Data Ingestion** â€” Collect and clean data from GDACS, GNews, and structured sources.  
3. **Feature Engineering** â€” Extract risk indicators and compute risk scores per route or supplier.  
4. **AI Risk Analysis** â€” Apply LLM-based summarization and predictive models.  
5. **Visualization** â€” Display interactive insights via Streamlit dashboard.  
6. **Automation** â€” Generate daily reports and refresh dashboards with new data.

---

## âš™ï¸ Local Development
```bash
pip install -r requirements.txt
streamlit run enhanced_streamlit_app.py
