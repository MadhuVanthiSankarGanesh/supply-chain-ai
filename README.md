# ğŸš¢ AI Supply Chain Intelligence Platform

An **end-to-end intelligent supply chain monitoring platform** built with **Azure Databricks, Azure OpenAI, and Streamlit**, providing real-time insights into logistics disruptions, route risks, and global trade vulnerabilities.

![Main Dashboard](assets/main_dashboard.png)

## ğŸŒ Overview

This platform combines **data engineering**, **AI-driven analytics**, and **interactive visualization** to assess and display supply chain risks.  
It continuously ingests event data (e.g., disasters, news, logistics updates), analyzes disruptions, and visualizes insights through an interactive Streamlit dashboard.

## âœ¨ Key Capabilities

- ğŸ¤– **AI-Powered Risk Analysis** â€” Uses Azure OpenAI to assess and summarize global risk signals  
- ğŸ—ºï¸ **Interactive Route Insights** â€” Real-time mapping and route-level risk analysis  
- ğŸ“Š **Dynamic Dashboards** â€” Streamlit-based visualization for ports, routes, and disasters  
- ğŸŒªï¸ **Event Monitoring** â€” Integration with live APIs for disaster and news tracking  
- ğŸ“„ **Automated Reporting** â€” Generates AI reports and route intelligence summaries  

## ğŸ§  Architecture Overview

```
[Databricks Notebooks] --> [Delta Tables / Processed Features]
           â†“
     [Azure OpenAI APIs]
           â†“
  [Streamlit Frontend Dashboard]
```

* **Databricks** â€” Handles data ingestion, processing, and ML feature creation
* **Azure OpenAI** â€” Provides LLM-based event summarization and report generation
* **Streamlit App** â€” Interactive user interface for visualization and exploration

## ğŸ§± Tech Stack

### Core Technologies

| Category                   | Tools Used                                                   |
| -------------------------- | ------------------------------------------------------------ |
| **Data Engineering**       | Azure Databricks, PySpark                                    |
| **AI & Analytics**         | Azure OpenAI API                                             |
| **Visualization**          | Streamlit, Plotly Express, Plotly Graph Objects              |
| **APIs / Data Sources**    | Custom REST APIs, GDACS (disaster data), GNews (news events) |
| **Environment Management** | Python 3.8+, pip, virtualenv                                 |

## ğŸ“˜ Databricks Notebooks

| Notebook                                            | Description                                                                        |
| --------------------------------------------------- | ---------------------------------------------------------------------------------- |
| ğŸ§© **01 â€“ Azure Databricks Environment Setup**      | Configures clusters, mounts, secrets, and connections to Azure and external APIs   |
| ğŸ”„ **02 â€“ Data Ingestion & Processing Pipeline**    | Collects and transforms data from APIs (GDACS, GNews) and structured datasets      |
| âš™ï¸ **03 â€“ Feature Engineering & Risk Analysis**     | Extracts risk features, computes indicators, and applies AI-based risk scoring     |
| â˜ï¸ **04 â€“ AI Agent Integration & Cloud Deployment** | Integrates Azure OpenAI and deploys models, enabling API and Streamlit connections |

All notebooks are located in the `databricks/` directory and can be imported into your Azure Databricks workspace.

## ğŸ¨ Streamlit Application

The Streamlit interface (`streamlit/enhanced_streamlit_app.py`) provides:

* An **AI Assistant** for conversational risk queries
* A **Risk Dashboard** with real-time port risk maps and metrics
* **Route Analysis** with AI-generated summaries and comparisons
* **Report Generation** for comprehensive daily or custom insights

### App Preview

| Section             | Screenshot                                   |
| ------------------- | -------------------------------------------- |
| ğŸ  Main Dashboard   | ![Main Dashboard](assets/main_dashboard.png) |
| ğŸ“Š Risk Dashboard   | ![Risk Dashboard](assets/risk_dashboard.png) |
| ğŸ—ºï¸ Route Analysis  | ![Route Analysis](assets/route_analysis.png) |
| ğŸ“„ Report Generator | ![Reports](assets/reports.png)               |

## ğŸ§© Project Workflow

1. **Environment Setup** â€” Configure Databricks cluster, secrets, and mounts
2. **Data Ingestion** â€” Collect & clean disaster/news/supply chain data
3. **Feature Engineering** â€” Compute route-level and regional risk metrics
4. **AI Risk Analysis** â€” Generate insights using Azure OpenAI models
5. **Visualization** â€” Display results via the Streamlit web interface
6. **Automation** â€” Generate on-demand and daily AI reports

## ğŸ“ Repository Structure

```
supply-chain-ai-platform/
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ 01_Environment_Setup.ipynb
â”‚   â”œâ”€â”€ 02_Data_Ingestion_Pipeline.ipynb
â”‚   â”œâ”€â”€ 03_FeatureEnginnering_RiskAnalysis.ipynb
â”‚   â””â”€â”€ 04_AIAgent_CloudDeployment.ipynb
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ enhanced_streamlit_app.py
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ main_dashboard.png
â”‚   â”œâ”€â”€ risk_dashboard.png
â”‚   â”œâ”€â”€ route_analysis.png
â”‚   â””â”€â”€ reports.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Installation & Local Development

### Prerequisites

* Python **3.8+**
* Access to **Azure Databricks**
* Access to **Azure OpenAI API**
* API keys for GDACS and GNews (optional if using sample data)

### Local Setup

```bash
# Clone repository
git clone https://github.com/<your-username>/supply-chain-ai-platform.git
cd supply-chain-ai-platform

# Install dependencies
pip install -r requirements.txt

# Add environment variables
cp .env.example .env
# Edit .env with your keys and endpoints

# Launch Streamlit app
streamlit run streamlit/enhanced_streamlit_app.py
```

## ğŸ”§ Configuration

Create a `.env` file with the following variables:

```
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
GDACS_API_KEY=your_gdacs_api_key
GNEWS_API_KEY=your_gnews_api_key
DATABRICKS_HOST=your_databricks_host
DATABRICKS_TOKEN=your_databricks_token
```

## ğŸš€ Databricks Deployment

1. Import all notebooks in the `databricks/` folder into your Azure Databricks workspace
2. Configure your cluster with the required libraries (PySpark, requests, etc.)
3. Mount external storage and set secrets for API keys
4. Execute notebooks sequentially:

   * `01_Environment_Setup.ipynb`
   * `02_Data_Ingestion_Pipeline.ipynb`
   * `03_FeatureEnginnering_RiskAnalysis.ipynb`
   * `04_AIAgent_CloudDeployment.ipynb`

## ğŸ“ˆ Performance Overview

| Metric                    | Description                                      |
| ------------------------- | ------------------------------------------------ |
| â±ï¸ **Processing Latency** | < 5 minutes for real-time event updates          |
| ğŸŒ **Data Coverage**      | 150+ countries and multiple event categories     |
| âš™ï¸ **Scalability**        | Handles millions of daily records via Databricks |
| ğŸ¤– **AI Integration**     | Dynamic risk summaries via Azure OpenAI          |


## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.


```

You can copy this entire content and paste it into a `README.md` file in your repository. All formatting, tables, code blocks, and structure will be preserved.
